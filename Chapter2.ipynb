{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Loading a Quantized Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spoilers\n",
    "\n",
    "In this chapter, we’ll:\n",
    "\n",
    "- Understand how quantization works\n",
    "- Explore the pros and cons of using different data types (FP16, BF16, FP32)\n",
    "- Introduce the concept of mixed-precision computing\n",
    "- Use BitsAndBytes to quantize a pretrained model while loading it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kODUm5BmEQhI",
    "outputId": "5a17ca8b-855a-4e55-b7a2-ab2af98d8906"
   },
   "outputs": [],
   "source": [
    "# If you're running on Colab\n",
    "!pip install datasets bitsandbytes trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're running on runpod.io's Jupyter Template\n",
    "#!pip install transformers peft huggingface-hub accelerate safetensors pandas matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from accelerate import init_empty_weights\n",
    "from accelerate.utils.modeling import find_tied_parameters, get_mixed_precision_context_manager\n",
    "from accelerate.utils.operations import convert_outputs_to_fp32\n",
    "from bitsandbytes.nn import Linear8bitLt, Linear4bit, LinearFP4, LinearNF4\n",
    "from collections import Counter\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer, AutoConfig\n",
    "from transformers.integrations.bitsandbytes import get_keys_to_not_convert\n",
    "from types import MethodType\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Goal\n",
    "\n",
    "We quantize models to reduce their memory footprint. We can easily shrink the model’s size to a quarter or an eighth of its original size. Keep in mind, however, that the more a model is quantized (i.e., the fewer bit used\n",
    "to represent its weights), the more likely its performance will be negatively affected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Reqs\n",
    "\n",
    "| Type | Name | # bits | Nickname |\n",
    "|---|---|---|---|\n",
    "| FP32 | Floating Point | 32 | Full Precision |\n",
    "| BF16 | Brain Float | 16 | Half-Precision |\n",
    "| FP16 | Floating Point | 16 | Half-Precision |\n",
    "| INT8 | Integer | 8 | 8-bit Quantized |\n",
    "| FP4 | Floating Point | 4 | 4-bit Quantized |\n",
    "| NF4 | Normal Float | 4 | 4-bit Quantized |\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch2/type_sizes.png?raw=True)\n",
    "<center>Figure 2.1 - Data type’s size comparison</center>\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch2/model_sizes.png?raw=True)\n",
    "<center>Figure 2.2 - Representing the same model using different data types</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization in a Nutshell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.2066), tensor(0.2097))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(11)\n",
    "weights = torch.randn(1000) * .07\n",
    "weights.min(), weights.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.2066, -0.1026,  0.0015,  0.1056,  0.2097]), tensor(0.1041))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_bins = 4\n",
    "bins = torch.linspace(weights.min(), weights.max(), n_bins+1)\n",
    "bin_width = bins[1]-bins[0]\n",
    "bins, bin_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 3))\n",
    "counts, _, _ = ax.hist(weights, bins=bins)\n",
    "ax.set_xlabel('Values')\n",
    "ax.set_title('Using Four Bins')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0358,  0.0720, -0.0247,  0.0086, -0.0127, -0.1048,  0.0099, -0.0367,\n",
      "        -0.0174, -0.0368,  0.2025, -0.0416,  0.0918,  0.0247, -0.0921, -0.0006,\n",
      "         0.0174,  0.1101, -0.1148, -0.1115])\n",
      "tensor([1, 2, 1, 2, 1, 0, 2, 1, 1, 1, 3, 1, 2, 2, 1, 1, 2, 3, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "bin_indexes = (weights.view(-1, 1) > bins).to(torch.int).argmin(dim=1) - 1\n",
    "print(weights[:20])\n",
    "print(bin_indexes[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 3))\n",
    "counts, _, _ = ax.hist(bin_indexes, bins=np.arange(n_bins+1)-.5)\n",
    "ax.set_xticks([0, 1, 2, 3])\n",
    "ax.set_xlabel('Bin Indexes')\n",
    "ax.set_title('Using Four Bins')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\Large\n",
    "\\text{n_bins}=2^{\\text{n_bits}} \\implies \\text{n_bits} = \\log_2({\\text{n_bins}})\n",
    "$$\n",
    "\n",
    "<center>Equation 2.1 - Number of bits vs number of bins</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2066, -0.1026,  0.0015,  0.1056])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_values = bins[:-1]\n",
    "first_bin = bin_values[0]\n",
    "bin_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\Large\n",
    "\\text{approx_value} = \\text{bin_index} * \\text{bin_width} + \\text{first_bin}\n",
    "$$\n",
    "\n",
    "<center>Equation 2.2 - Retrieving the (approximate) original value</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2066, -0.1026,  0.0015,  0.1056])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, n_bins) * bin_width + first_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1026,  0.0015, -0.1026,  0.0015, -0.1026, -0.2066,  0.0015, -0.1026,\n",
      "        -0.1026, -0.1026,  0.1056, -0.1026,  0.0015,  0.0015, -0.1026, -0.1026,\n",
      "         0.0015,  0.1056, -0.2066, -0.2066])\n"
     ]
    }
   ],
   "source": [
    "approx_values = bin_indexes * bin_width + first_bin\n",
    "print(approx_values[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0358,  0.0720, -0.0247,  0.0086, -0.0127, -0.1048,  0.0099, -0.0367,\n",
      "        -0.0174, -0.0368,  0.2025, -0.0416,  0.0918,  0.0247, -0.0921, -0.0006,\n",
      "         0.0174,  0.1101, -0.1148, -0.1115])\n"
     ]
    }
   ],
   "source": [
    "print(weights[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0615)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "mse_fn = nn.MSELoss()\n",
    "mse_fn(approx_values, weights).sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize(weights, n_bits=8):\n",
    "    assert n_bits <= 16, \"Using more bits may very slow execution and/or crashing.\"\n",
    "    n_bins = 2**n_bits\n",
    "    bins = torch.linspace(weights.min(), weights.max(), n_bins+1)\n",
    "    first_bin = bins[0]\n",
    "    bin_width = bins[1]-bins[0]\n",
    "    bin_indexes = (weights.view(-1, 1) > bins).to(torch.int).argmin(dim=1) - 1\n",
    "    return bin_indexes, bin_width, first_bin\n",
    "\n",
    "def dequantize(bin_indexes, bin_width, first_bin):\n",
    "    approx_values = bin_indexes * bin_width + first_bin\n",
    "    return approx_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-bit Quantization:\n",
      "tensor([-0.1026,  0.0015, -0.1026,  0.0015, -0.1026, -0.2066])\n",
      "tensor([-0.0358,  0.0720, -0.0247,  0.0086, -0.0127, -0.1048])\n",
      "tensor(0.0615)\n",
      "\n",
      "\n",
      "4-bit Quantization:\n",
      "tensor([-0.0505,  0.0535, -0.0505,  0.0015, -0.0245, -0.1286])\n",
      "tensor([-0.0358,  0.0720, -0.0247,  0.0086, -0.0127, -0.1048])\n",
      "tensor(0.0152)\n",
      "\n",
      "\n",
      "8-bit Quantization:\n",
      "tensor([-0.0359,  0.0714, -0.0261,  0.0080, -0.0131, -0.1058])\n",
      "tensor([-0.0358,  0.0720, -0.0247,  0.0086, -0.0127, -0.1048])\n",
      "tensor(0.0010)\n",
      "\n",
      "\n",
      "16-bit Quantization:\n",
      "tensor([-0.0359,  0.0718, -0.0248,  0.0085, -0.0128, -0.1049])\n",
      "tensor([-0.0358,  0.0720, -0.0247,  0.0086, -0.0127, -0.1048])\n",
      "tensor(0.0001)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n_bits in [2, 4, 8, 16]:\n",
    "    res = quantize(weights, n_bits=n_bits)\n",
    "    approx_values = dequantize(*res)\n",
    "    print(f'{n_bits}-bit Quantization:')\n",
    "    print(approx_values[:6])\n",
    "    print(weights[:6])\n",
    "    print(mse_fn(approx_values, weights).sqrt())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "**ASIDE: Weight Distribution of Phi-3's Linear Layers**\n",
    "\n",
    "The following plots show the weight distribution of a large linear layer, `qkv_proj`, within the self-\n",
    "attention block in Phi-3. Other layers, such as `o_proj`, also located within the self-attention block, and\n",
    "`gate_up_proj` and `down_proj`, in the MLP block, have very similar weight distributions. This layer is\n",
    "present in every one of the 32 decoder blocks (indicated by the number in square brackets). You’ll notice\n",
    "that these millions of weights are concentrated within a very narrow range. But there are a few outliers as\n",
    "well, so each subplot also contains the actual range of observed weights in the corresponding layer.\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch2/self_attn.qkv_proj.png?raw=True)\n",
    "<center>Figure 2.5 - Weight distribution of Phi-3 layers</center>\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Half-Precision Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0358,  0.0720, -0.0247,  0.0086, -0.0127, -0.1048],\n",
      "       dtype=torch.float16)\n",
      "tensor([-0.0358,  0.0720, -0.0247,  0.0086, -0.0127, -0.1048])\n"
     ]
    }
   ],
   "source": [
    "fp16_weights = weights.to(torch.float16)\n",
    "print(fp16_weights[:6])\n",
    "print(weights[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4244e-05)\n"
     ]
    }
   ],
   "source": [
    "print(mse_fn(fp16_weights, weights).sqrt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Living on the Edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8526e-16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(14)\n",
    "tiny_values = torch.randn(1000)*1e-5\n",
    "fp16_tiny_values = tiny_values.to(torch.float16)\n",
    "mse_fn(fp16_tiny_values, tiny_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.7241e-06,  1.1441e-05,  3.7199e-06, -1.1252e-06, -2.4735e-08])\n",
      "tensor([-2.7418e-06,  1.1444e-05,  3.6955e-06, -1.1325e-06, -0.0000e+00],\n",
      "       dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(tiny_values[155:160])\n",
    "print(fp16_tiny_values[155:160])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([155074.0938,  64881.6602,   2729.5815, -40790.6562,  68846.7188])\n",
      "tensor([    inf,  64896.,   2730., -40800.,     inf], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(19)\n",
    "large_values = torch.randn(1000)*1e5\n",
    "fp16_large_values = large_values.to(torch.float16)\n",
    "print(large_values[:5])\n",
    "print(fp16_large_values[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finfo(resolution=0.001, min=-65504, max=65504, eps=0.000976562, smallest_normal=6.10352e-05, tiny=6.10352e-05, dtype=float16)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp16_info = torch.finfo(torch.float16)\n",
    "fp16_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.960464477539063e-08"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smallest_subnormal = fp16_info.smallest_normal * 2**-10\n",
    "smallest_subnormal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Brain Float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finfo(resolution=0.01, min=-3.38953e+38, max=3.38953e+38, eps=0.0078125, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=bfloat16)\n",
      "finfo(resolution=0.001, min=-65504, max=65504, eps=0.000976562, smallest_normal=6.10352e-05, tiny=6.10352e-05, dtype=float16)\n"
     ]
    }
   ],
   "source": [
    "bf16_info = torch.finfo(torch.bfloat16)\n",
    "print(bf16_info)\n",
    "print(fp16_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finfo(resolution=1e-06, min=-3.40282e+38, max=3.40282e+38, eps=1.19209e-07, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp32_info = torch.finfo(torch.float32)\n",
    "fp32_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.555555582])\n",
      "tensor([0.555664062], dtype=torch.float16)\n",
      "tensor([0.554687500], dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([0.555555555])\n",
    "torch.set_printoptions(precision=9)\n",
    "print(x)\n",
    "print(x.to(torch.float16))\n",
    "print(x.to(torch.bfloat16))\n",
    "torch.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Type | Precision | Sub-normal | Min. | Max. |\n",
    "|---|---|---|---|---|\n",
    "|FP32 | e-08 | e-45 | e-38 | e+38 |\n",
    "|BF16 | e-03  | NA | e-38 | e+38 |\n",
    "|FP16 | e-04  | e-08  | e-05 | e+04 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Models\n",
    "\n",
    "****\n",
    "**Summary of \"Loading Models\"**\n",
    "- if supported by your GPU, use `torch.bfloat16` instead of `torch.float16` for all things 16-bit\n",
    " ```python\n",
    " supported = torch.cuda.is_bf16_supported(including_emulation=False)\n",
    " dtypes16 = (torch.bfloat16 if supported else torch.float16)\n",
    " ```\n",
    "- when loading a pretrained model, always specify its `torch_dype` upfront\n",
    " ```python\n",
    " model = AutoModelForCausalLM.from_pretrained(repo_id, device_map='auto', torch_dtype=torch.float32)\n",
    " ```\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parm_dtypes(iterable, top_k=3):\n",
    "    return Counter([p.dtype for p in iterable]).most_common(top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1324.785664\n",
      "[(torch.float32, 388)]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", device_map='auto')\n",
    "print(model.get_memory_footprint()/1e6)   \n",
    "print(get_parm_dtypes(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 dvgodoy dvgodoy 662513657 May 11  2022 pytorch_model.bin\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/facebook/opt-350m/resolve/main/pytorch_model.bin\n",
    "!ls -la pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(torch.float16, 388)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load('pytorch_model.bin')\n",
    "get_parm_dtypes(iter(state_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", \n",
    "                                             device_map='auto',\n",
    "                                             torch_dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "batch = tokenizer(['This is a simple test'], return_tensors='pt')\n",
    "batch['labels'] = batch['input_ids']\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch = {k: v.to(device) for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.8001, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(**batch)\n",
    "out.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Half-Precision Models (16-bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supported = torch.cuda.is_bf16_supported(including_emulation=False)\n",
    "dtype16 = (torch.bfloat16 if supported else torch.float16)\n",
    "dtype16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "662.392832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(torch.float16, 388)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(dtype16)\n",
    "print(model.get_memory_footprint()/1e6)\n",
    "get_parm_dtypes(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "662.392832\n",
      "[(torch.float16, 388)]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", \n",
    "                                             device_map='auto',\n",
    "                                             torch_dtype=dtype16)\n",
    "print(model.get_memory_footprint()/1e6)\n",
    "print(get_parm_dtypes(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.8008, device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(**batch)\n",
    "out.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedModel(nn.Module):\n",
    "    def __init__(self, dtype):\n",
    "        super().__init__()\n",
    "        self.a = nn.Linear(1000, 1000, dtype=dtype)\n",
    "        self.b = nn.Linear(1000, 1000, dtype=dtype)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.b(self.a(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixedModel(\n",
       "  (a): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "  (b): Linear(in_features=1000, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed32 = MixedModel(torch.float32)\n",
    "mixed32.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.41 ms ± 28 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit mixed32(torch.randn(1000, 1000, dtype=torch.float32, device='cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixedModel(\n",
       "  (a): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "  (b): Linear(in_features=1000, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed16 = MixedModel(torch.float16)\n",
    "mixed16.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248 µs ± 1.35 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit mixed16(torch.randn(1000, 1000, dtype=torch.float16, device='cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277 µs ± 1.29 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "    %timeit mixed32(torch.randn(1000, 1000, dtype=torch.float32, device='cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "    res16 = mixed32(torch.randn(1000, 1000, dtype=torch.float32, device='cuda'))\n",
    "    \n",
    "res32 = res16.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocast_context = torch.autocast(device_type=\"cuda\", dtype=torch.float16)\n",
    "# original forward method\n",
    "model_forward_func = mixed32.forward.__func__\n",
    "# wrapping the method with the context manager\n",
    "new_forward = autocast_context(model_forward_func)\n",
    "# assigning the wrapped method back to the model\n",
    "mixed32.forward = MethodType(new_forward, mixed32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = mixed32(torch.randn(1000, 1000, dtype=torch.float32, device='cuda'))\n",
    "res.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed32.forward = MethodType(convert_outputs_to_fp32(mixed32.forward.__func__), mixed32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = mixed32(torch.randn(1000, 1000, dtype=torch.float32, device='cuda'))\n",
    "res.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371 µs ± 1.27 µs per loop  (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit mixed32(torch.randn(1000, 1000, dtype=torch.float32, device='cuda'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BitsAndBytes\n",
    "\n",
    "[BitsAndBytes](https://huggingface.co/docs/bitsandbytes/main/en/index) is your go-to package for quantization. From its documentation:\n",
    "\n",
    "\"_bitsandbytes enables accessible large language models via k-bit quantization for PyTorch. bitsandbytes provides three main features for dramatically reducing memory consumption for inference and training:_\n",
    "\n",
    "- _8-bit optimizers uses block-wise quantization to maintain 32-bit performance at a small fraction of the memory cost._\n",
    "- _LLM.Int() or 8-bit quantization enables large language model inference with only half the required memory and without any performance degradation. This method is based on vector-wise quantization to quantize most features to 8-bits and separately treating outliers with 16-bit matrix multiplication._\n",
    "- _QLoRA or 4-bit quantization enables large language model training with several memory-saving techniques that don’t compromise performance. This method quantizes a model to 4-bits and inserts a small set of trainable low-rank adaptation (LoRA) weights to allow training._\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BitsAndBytesConfig {\n",
       "  \"_load_in_4bit\": false,\n",
       "  \"_load_in_8bit\": false,\n",
       "  \"bnb_4bit_compute_dtype\": \"float32\",\n",
       "  \"bnb_4bit_quant_storage\": \"uint8\",\n",
       "  \"bnb_4bit_quant_type\": \"fp4\",\n",
       "  \"bnb_4bit_use_double_quant\": false,\n",
       "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "  \"llm_int8_has_fp16_weight\": false,\n",
       "  \"llm_int8_skip_modules\": null,\n",
       "  \"llm_int8_threshold\": 6.0,\n",
       "  \"load_in_4bit\": false,\n",
       "  \"load_in_8bit\": false,\n",
       "  \"quant_method\": \"bitsandbytes\"\n",
       "}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig()\n",
    "bnb_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8-Bit Quantization\n",
    "\n",
    "\"_LLM.int8() is a quantization method that doesn’t degrade performance which makes large model inference more accessible. The key is to extract the outliers from the inputs and weights and multiply them in 16-bit. All other values are multiplied in 8-bit and quantized to Int8 before being dequantized back to 16-bits. The outputs from the 16-bit and 8-bit multiplication are combined to produce the final output._\"\n",
    "\n",
    "Source: [8-bit quantization](https://huggingface.co/docs/bitsandbytes/en/reference/nn/linear8bit)\n",
    "\n",
    "****\n",
    "**Summary of \"8-Bit Quantization\"**\n",
    "- load an 8-bit quantized model in a few lines of code:\n",
    "  ```python\n",
    "    bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(repo_id, \n",
    "                                                 device_map='auto',\n",
    "                                                 torch_dtype=torch.float32,\n",
    "                                                 quantization_config=bnb_config)\n",
    "  ```\n",
    "  - quantization modifies the default type of non-quantized layers to `torch.float16` unless we actively provide the `torch_dtype` argument when calling the `from_pretrained()` method\n",
    "- 8-bit quantization replaces all linear layers except for:\n",
    "  - layers with tied (shared) weights\n",
    "  - the last layer in the model\n",
    "  - any layer named `lm_head`\n",
    "- if you want to skip additional modules, use the `llm_int8_skip_modules` configuration argument and make sure to manually include the layers with tied (shared) weights to avoid errors\n",
    "- computation (inside the quantized layers) happens in `torch.float16`\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359.354368\n",
      "[(torch.float16, 242), (torch.int8, 146)]\n"
     ]
    }
   ],
   "source": [
    "bnb_config_q8 = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model_q8 = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\",\n",
    "                                                device_map='auto',\n",
    "                                                quantization_config=bnb_config_q8)\n",
    "print(model_q8.get_memory_footprint()/1e6)\n",
    "print(get_parm_dtypes(model_q8.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you may not get a NaN back, as it depends on the environment\n",
    "out = model_q8(**batch)\n",
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415.670272\n",
      "[(torch.float32, 242), (torch.int8, 146)]\n"
     ]
    }
   ],
   "source": [
    "model_q8_32 = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", \n",
    "                                                device_map='auto',\n",
    "                                                quantization_config=bnb_config_q8,\n",
    "                                                torch_dtype=torch.float32)\n",
    "print(model_q8_32.get_memory_footprint()/1e6)\n",
    "print(get_parm_dtypes(model_q8_32.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.8024, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model_q8_32(**batch)\n",
    "out.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quantized Linear Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTDecoderLayer(\n",
       "  (self_attn): OPTAttention(\n",
       "    (k_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
       "    (v_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
       "    (q_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
       "    (out_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (activation_fn): ReLU()\n",
       "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc1): Linear8bitLt(in_features=1024, out_features=4096, bias=True)\n",
       "  (fc2): Linear8bitLt(in_features=4096, out_features=1024, bias=True)\n",
       "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_layer = model_q8_32.model.decoder.layers[0]\n",
    "dec_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear8bitLt(in_features=1024, out_features=1024, bias=True)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q8_layer = dec_layer.self_attn.k_proj\n",
    "q8_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[ -67, -113,  -89,  ...,   65,  -16,  -87],\n",
       "                      [  60,  120,   90,  ...,  -50,   32,   80],\n",
       "                      [  47,  127,   86,  ...,  -34,    8,   90],\n",
       "                      ...,\n",
       "                      [ -65,   65,   34,  ...,  -64,   35,   64],\n",
       "                      [  57,   67,   21,  ...,   63,  -64,  -64],\n",
       "                      [ -64,   63,  -11,  ...,  -64,   34,   63]], device='cuda:0',\n",
       "                     dtype=torch.int8)),\n",
       "             ('bias',\n",
       "              tensor([-0.0134,  0.0082,  0.0161,  ..., -0.0242, -0.0150,  0.0203],\n",
       "                     device='cuda:0')),\n",
       "             ('SCB',\n",
       "              tensor([0.1250, 0.1252, 0.1250,  ..., 0.1252, 0.1250, 0.1254], device='cuda:0')),\n",
       "             ('weight_format', tensor(0, dtype=torch.uint8))])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q8_state = q8_layer.state_dict()\n",
    "q8_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(50272, 512, padding_idx=1)\n",
      "Linear(in_features=512, out_features=50272, bias=False)\n"
     ]
    }
   ],
   "source": [
    "print(model.model.decoder.embed_tokens)\n",
    "print(model.lm_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(model.model.decoder.embed_tokens.weight, \n",
    "               model.lm_head.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, [['lm_head.weight', 'model.decoder.embed_tokens.weight']])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained('facebook/opt-350m')\n",
    "\n",
    "config.tie_word_embeddings, find_tied_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(..., device='meta', size=(50272, 512), requires_grad=True)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with init_empty_weights(): # loads meta tensors only\n",
    "    empty_model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "empty_model.lm_head.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lm_head', 'model.decoder.embed_tokens']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_modules = get_keys_to_not_convert(empty_model)\n",
    "skip_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm_head: torch.float32\n",
      "model.decoder.embed_tokens: torch.float32\n"
     ]
    }
   ],
   "source": [
    "for module in skip_modules:\n",
    "    print(f'{module}: {next(model_q8_32.get_submodule(module).parameters()).dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `llm_int8_skip_modules`\n",
    "\n",
    "If your model has tied weights, and you choose to use your own list of modules to skip, you\n",
    "must add one of the tied layers to your list. If you don’t, you may get the following\n",
    "exception:\n",
    "\n",
    "***\n",
    "`AttributeError: 'Parameter' object has no attribute 'SCB'`\n",
    "***\n",
    "\n",
    "```python\n",
    "# This configuration WILL raise an exception \n",
    "# while trying to load weights for the tied layer\n",
    "# bnb_config_skip = BitsAndBytesConfig(load_in_8bit=True, \n",
    "#                                      llm_int8_skip_modules=['o_proj'])\n",
    "\n",
    "# This configuration works fine because \n",
    "# the tied layer, lm_head, is in the list\n",
    "bnb_config_skip = BitsAndBytesConfig(\n",
    "        load_in_8bit=True, \n",
    "        llm_int8_skip_modules=['o_proj', 'lm_head'])\n",
    "\n",
    "model_skip = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\",\n",
    "                                                  device_map='auto',\n",
    "                                                  torch_dtype=torch.float32,\n",
    "                                                  quantization_config=bnb_config_skip)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8-bit Layers\n",
    "\n",
    "\"*In order to quantize a linear layer one should first load the original fp16 / bf16 weights into the Linear8bitLt module, then call int8_module.to(\"cuda\") to quantize the fp16 weights.*\"\n",
    "\n",
    "Source: [8-bit quantization](https://huggingface.co/docs/bitsandbytes/en/reference/nn/linear8bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[-0.2220, -0.0085,  0.3072, -0.2097,  0.0531,  0.1224,  0.0525, -0.2350,\n",
       "                        0.0456,  0.2687],\n",
       "                      [-0.1459,  0.1786, -0.1443, -0.0233,  0.1689,  0.0015, -0.2514,  0.1644,\n",
       "                        0.1920,  0.1678],\n",
       "                      [ 0.2346,  0.1411,  0.2128,  0.0519,  0.2147, -0.2786, -0.0433, -0.0364,\n",
       "                       -0.1504,  0.0823],\n",
       "                      [ 0.2388, -0.2134, -0.1620, -0.1023,  0.2433, -0.2680,  0.3099, -0.1933,\n",
       "                       -0.0471, -0.0391],\n",
       "                      [-0.1273,  0.2197, -0.0136, -0.1938, -0.1746,  0.0404,  0.0711, -0.1730,\n",
       "                        0.0539, -0.1992],\n",
       "                      [-0.0051,  0.1373, -0.0267, -0.0907, -0.0107,  0.1108, -0.1566,  0.0172,\n",
       "                        0.2075, -0.0028],\n",
       "                      [ 0.2082, -0.2857, -0.2640, -0.1436,  0.1704,  0.1908, -0.2350,  0.1187,\n",
       "                       -0.0568,  0.0916],\n",
       "                      [ 0.2974, -0.3061,  0.0559,  0.1899,  0.0265, -0.1893, -0.0582, -0.0943,\n",
       "                        0.2451,  0.2825],\n",
       "                      [-0.1241, -0.3106, -0.1002, -0.1745,  0.2693,  0.2985,  0.1633, -0.0270,\n",
       "                       -0.3049,  0.0227],\n",
       "                      [-0.1217,  0.0035, -0.1481, -0.0330,  0.1787,  0.3123,  0.2600, -0.1720,\n",
       "                        0.2059,  0.2057]])),\n",
       "             ('bias',\n",
       "              tensor([ 0.1269,  0.2999,  0.0252, -0.0380, -0.1788, -0.0704,  0.1124, -0.2233,\n",
       "                       0.0653, -0.0854]))])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_in = 10\n",
    "n_out = 10\n",
    "\n",
    "torch.manual_seed(11)\n",
    "fp_layer = nn.Linear(n_in, n_out)\n",
    "\n",
    "int8_layer = Linear8bitLt(n_in, n_out, has_fp16_weights=False)\n",
    "\n",
    "int8_layer.load_state_dict(fp_layer.state_dict())\n",
    "int8_layer.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[ -92,   -4,  127,  -87,   22,   51,   22,  -97,   19,  111],\n",
       "                      [ -74,   90,  -73,  -12,   85,    1, -127,   83,   97,   85],\n",
       "                      [ 107,   64,   97,   24,   98, -127,  -20,  -17,  -69,   38],\n",
       "                      [  98,  -87,  -66,  -42,  100, -110,  127,  -79,  -19,  -16],\n",
       "                      [ -74,  127,   -8, -112, -101,   23,   41, -100,   31, -115],\n",
       "                      [  -3,   84,  -16,  -56,   -7,   68,  -96,   11,  127,   -2],\n",
       "                      [  93, -127, -117,  -64,   76,   85, -104,   53,  -25,   41],\n",
       "                      [ 123, -127,   23,   79,   11,  -79,  -24,  -39,  102,  117],\n",
       "                      [ -51, -127,  -41,  -71,  110,  122,   67,  -11, -125,    9],\n",
       "                      [ -50,    1,  -60,  -13,   73,  127,  106,  -70,   84,   84]],\n",
       "                     device='cuda:0', dtype=torch.int8)),\n",
       "             ('bias',\n",
       "              tensor([ 0.1269,  0.2999,  0.0252, -0.0380, -0.1788, -0.0704,  0.1124, -0.2233,\n",
       "                       0.0653, -0.0854], device='cuda:0')),\n",
       "             ('SCB',\n",
       "              tensor([0.3071, 0.2515, 0.2786, 0.3098, 0.2197, 0.2075, 0.2856, 0.3062, 0.3105,\n",
       "                      0.3123], device='cuda:0')),\n",
       "             ('weight_format', tensor(0, dtype=torch.uint8))])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int8_layer = int8_layer.to(0) # Quantization happens here\n",
    "int8_state = int8_layer.state_dict()\n",
    "int8_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-bit Quantization\n",
    "\n",
    "\"*QLoRA is a finetuning method that quantizes a model to 4-bits and adds a set of low-rank adaptation (LoRA) weights to the model and tuning them through the quantized weights. This method also introduces a new data type, 4-bit NormalFloat (LinearNF4) in addition to the standard Float4 data type (LinearFP4). LinearNF4 is a quantization data type for normally distributed data and can improve performance.*\"\n",
    "\n",
    "Source: [4-bit quantization](https://huggingface.co/docs/bitsandbytes/en/reference/nn/linear4bit)\n",
    "\n",
    "****\n",
    "**Summary of \"4-Bit Quantization\"**\n",
    "- squeeze the most of a 4-bit quantized model by using the normal float (NF4) type and double quantization\n",
    "  ```python\n",
    "  supported = torch.cuda.is_bf16_supported(including_emulation=False)\n",
    "  compute_dtype = (torch.bfloat16 if supported else torch.float32)\n",
    "  nf4_config = BitsAndBytesConfig(\n",
    "     load_in_4bit=True,\n",
    "     bnb_4bit_quant_type=\"nf4\",\n",
    "     bnb_4bit_use_double_quant=True,\n",
    "     bnb_4bit_compute_dtype=compute_dtype\n",
    "  )\n",
    "  model = AutoModelForCausalLM.from_pretrained(repo_id, \n",
    "                                               device_map='auto',\n",
    "                                               torch_dtype=torch.float32,\n",
    "                                               quantization_config=nf4_config)\n",
    "  ```\n",
    "- computation happens (inside the quantized layers) in the specified type (`bnb_4bit_compute_dtype`):\n",
    "FP32 is better than BF16, which is better than FP16.\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "supported = torch.cuda.is_bf16_supported(including_emulation=False)\n",
    "compute_dtype = (torch.bfloat16 if supported else torch.float32)\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=compute_dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Secret Lives of `Dtypes`**\n",
    "\n",
    "| Regular Model | Quantized Model |\n",
    "|---|---|\n",
    "| ![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch2/type_flow_regular.png?raw=True) | ![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch2/type_flow_qt.png?raw=True) |\n",
    "| <center>Figure 2.6 - Data types flowing through a regular model</center> | <center>Figure 2.7 - Data types flowing through a quantized model</center> |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264.15104\n",
      "[(torch.float32, 242), (torch.uint8, 146)]\n"
     ]
    }
   ],
   "source": [
    "model_q4 = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", \n",
    "                                                device_map='auto',\n",
    "                                                torch_dtype=torch.float32,\n",
    "                                                quantization_config=nf4_config)\n",
    "print(model_q4.get_memory_footprint()/1e6)\n",
    "print(get_parm_dtypes(model_q4.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.7016, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model_q4(**batch)\n",
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTDecoderLayer(\n",
       "  (self_attn): OPTAttention(\n",
       "    (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "    (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "    (q_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "    (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (activation_fn): ReLU()\n",
       "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
       "  (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
       "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_layer = model_q4.model.decoder.layers[0]\n",
    "dec_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear4bit(in_features=1024, out_features=1024, bias=True)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q4_layer = dec_layer.self_attn.k_proj\n",
    "q4_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[ 32],\n",
       "                      [ 29],\n",
       "                      [208],\n",
       "                      ...,\n",
       "                      [ 66],\n",
       "                      [ 34],\n",
       "                      [172]], device='cuda:0', dtype=torch.uint8)),\n",
       "             ('bias',\n",
       "              tensor([-0.0134,  0.0082,  0.0161,  ..., -0.0242, -0.0150,  0.0203],\n",
       "                     device='cuda:0', dtype=torch.float16)),\n",
       "             ('weight.absmax',\n",
       "              tensor([230, 230,  30,  ...,   1,  26, 191], device='cuda:0',\n",
       "                     dtype=torch.uint8)),\n",
       "             ('weight.quant_map',\n",
       "              tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
       "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
       "                     device='cuda:0')),\n",
       "             ('weight.nested_absmax',\n",
       "              tensor([0.0077, 0.0142, 0.0153, 0.0138, 0.0399, 0.0409, 0.0417, 0.0426, 0.0053,\n",
       "                      0.0053, 0.0053, 0.0053, 0.0051, 0.0051, 0.0051, 0.0053, 0.0195, 0.0269,\n",
       "                      0.0223, 0.0195, 0.0053, 0.0053, 0.0054, 0.0054, 0.0317, 0.0315, 0.0306,\n",
       "                      0.0320, 0.0262, 0.0265, 0.0230, 0.0296, 0.0051, 0.0065, 0.0050, 0.0050,\n",
       "                      0.0051, 0.0053, 0.0050, 0.0051, 0.0056, 0.0056, 0.0056, 0.0056, 0.0428,\n",
       "                      0.0410, 0.0405, 0.0422, 0.0397, 0.0402, 0.0394, 0.0414, 0.0204, 0.0106,\n",
       "                      0.0140, 0.0078, 0.0406, 0.0349, 0.0359, 0.0418, 0.0503, 0.0499, 0.0470,\n",
       "                      0.0458], device='cuda:0')),\n",
       "             ('weight.nested_quant_map',\n",
       "              tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
       "                      -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
       "                      -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
       "                      -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
       "                      -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
       "                      -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
       "                      -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
       "                      -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
       "                      -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
       "                      -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
       "                      -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
       "                      -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
       "                      -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
       "                      -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
       "                      -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
       "                      -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
       "                      -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
       "                      -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
       "                      -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
       "                      -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
       "                      -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
       "                      -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
       "                      -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
       "                      -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
       "                      -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
       "                      -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
       "                       7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
       "                       1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
       "                       7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
       "                       2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
       "                       5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
       "                       8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
       "                       1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
       "                       2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
       "                       4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
       "                       5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
       "                       7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
       "                       8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
       "                       9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
       "                       1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
       "                       2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
       "                       3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
       "                       3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
       "                       4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
       "                       5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
       "                       5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
       "                       6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
       "                       7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
       "                       7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
       "                       8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
       "                       9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
       "                       1.0000e+00], device='cuda:0')),\n",
       "             ('weight.quant_state.bitsandbytes__nf4',\n",
       "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
       "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
       "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
       "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
       "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
       "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
       "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
       "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
       "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
       "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
       "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  49,\n",
       "                       49,  57,  57,  56,  49,  55,  50,  49,  48,  52,  51,  53,  56,  54,\n",
       "                       55,  51, 125], dtype=torch.uint8))])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q4_layer.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FP4 vs NF4 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=10, out_features=10, bias=True)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_in = 10\n",
    "n_out = 10\n",
    "torch.manual_seed(11)\n",
    "fp16_layer = nn.Linear(n_in, n_out)\n",
    "fp16_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp4_layer = LinearFP4(n_in, n_out)\n",
    "fp4_layer.load_state_dict(fp16_layer.state_dict())\n",
    "\n",
    "nf4_model = LinearNF4(n_in, n_out)\n",
    "nf4_model.load_state_dict(fp16_layer.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0000,  0.0052,  0.6667,  1.0000,  0.3333,  0.5000,  0.1667,  0.2500,\n",
       "          0.0000, -0.0052, -0.6667, -1.0000, -0.3333, -0.5000, -0.1667, -0.2500],\n",
       "        device='cuda:0'),\n",
       " torch.Size([50, 1]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp4_layer = fp4_layer.to(0) # Quantization happens here\n",
    "fp4_state = fp4_layer.state_dict()\n",
    "\n",
    "fp4_state['weight.quant_map'], fp4_state['weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'FP4 quantization')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x50 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, .5))\n",
    "ax.scatter(x=sorted(fp4_state['weight.quant_map'].tolist()), y=[0]*16)\n",
    "ax.set_yticks([])\n",
    "ax.set_title('FP4 quantization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
       "          0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
       "        device='cuda:0'),\n",
       " torch.Size([50, 1]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nf4_model = nf4_model.to(0) # Quantization happens here\n",
    "nf4_state = nf4_model.state_dict()\n",
    "\n",
    "nf4_state['weight.quant_map'], nf4_state['weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'NF4 quantization')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x50 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, .5))\n",
    "ax.scatter(x=sorted(nf4_state['weight.quant_map'].tolist()), y=[0]*16)\n",
    "ax.set_yticks([])\n",
    "ax.set_title('NF4 quantization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coming Up in \"Fine-Tuning LLMs\"\n",
    "\n",
    "As huge linear layers are being replaced by their quantized versions to reduce the model’s memory footprint, a\n",
    "new issue arises. These quantized layers cannot be easily updated, thus rendering fine-tuning next to\n",
    "impossible. Could a new kind of layer be the solution to this conundrum? Find out in the next thrilling chapter\n",
    "of \"Fine-Tuning LLMs.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
